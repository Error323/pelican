/**

@page user_framework_deploymentExamples Deployment Configurations of Pelican

@image html dataFlow.png "Data Flow Components in The Pelican Framework"
@image latex dataFlow.eps "Data Flow Components in The Pelican Framework"

A pelican application is designed to allow the flexible deployment of hardware resources to
cope with the data rates and data processing requirements of an incoming data stream. 

Within the framework the data flow is split three distinct logical units with well defined interfaces between them. 

These units are Data Input (with the \p AbstractDataClient interface),
Pipeline Processing (with the \p AbstractPipeline interface), and 
Data Output( with the \p AbstractDataBlobStreamer interface)

Each unit can be configured differently to give maximal flexibility in deployment.
In this way, you can insert extra hardware to relieve bottle necks in processing and/or 
bandwidth.

@image html dataInput.png
@image latex dataInput.eps
@section user_framework_deployment_dataInput Data Import Configurations

Data from the source can be either piped directly into the pipeline or via
a Pelican Server. It is merely a question of specifying the type of Data Client required.
The data clients job is to keep the pipeline supplied with data to process whenever it becomes idle.

@subsection user_framework_deployment_dataStreamingClient The DataStreamingClient

This sceanario assumes that the entire processing can be performed on a single machine.

The DataStreamingClient allows you to connect to one or more incoming streams on the same
machine on which processing occurs, without an intervening data distribution server.
@todo example configuration
@code
@endcode


@subsection user_framework_deployment_PelicanClient Pelican Server

This sceanario is useful when you are processing limited. You can extend the number of
processing units to keep up with the incomming data stream.

The Pelican Server attaches to the incoming data stream and chops this into chunks of
self-contained data for processing in parallel. The pipelines then connect to the server
to download a data chunk to process.
@todo example configuration
@code
@endcode

@image html dataOutput.png
@image latex dataOutput.eps
@section user_framework_deployment_dataOutput Data Output Configurations

Data can be sent for output streaming at any time in the pipeline by calling an appropriate
@p OutputStreamer module. The available streaming types are

@li TCP Server running the pelican protocol
@todo example configuration
@code
@endcode
*/
